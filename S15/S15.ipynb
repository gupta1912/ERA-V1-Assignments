{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install datasets\n",
    "# !pip install torchmetrics \n",
    "# !pip install tensorboard \n",
    "# !pip install torchtext\n",
    "# !pip install tokenizers\n",
    "# !pip install -U rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import ModelSummary, RichProgressBar\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn \n",
    "import random\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from datasets import load_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_transformer\n",
    "from config import get_config \n",
    "from dataset import BilingualDataset\n",
    "from train import greedy_decode_pl, get_or_build_tokenizer, get_ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lit_Transformer(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.config = get_config()\n",
    "        _, _, self.tokenizer_src, self.tokenizer_tgt = get_ds(self.config)\n",
    "        self.model = build_transformer(self.tokenizer_src.get_vocab_size(), self.tokenizer_tgt.get_vocab_size(), \n",
    "                                       self.config[\"seq_len\"], self.config[\"seq_len\"], d_model=self.config[\"d_model\"])\n",
    "        self.loss_fn =  nn.CrossEntropyLoss(ignore_index=self.tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1)\n",
    "        self.max_len = self.config['seq_len']\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_input = x['encoder_input'] # (B, seq_len)\n",
    "        decoder_input = x['decoder_input'] # (B, seq_len) \n",
    "        encoder_mask = x['encoder_mask'] # (B, 1, 1, seq_len) \n",
    "        decoder_mask = x['decoder_mask'] # (B, 1, seq_len,-seq_len) \n",
    "            \n",
    "        # Run the tensors through the encoder, decoder and the projection layer \n",
    "        encoder_output = self.model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model) \n",
    "        decoder_output = self.model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) \n",
    "        proj_output = self.model.project(decoder_output) # (B, seq_len, vocab_size) \n",
    "\n",
    "        return proj_output\n",
    "    \n",
    "    def forward_inference(self, x): \n",
    "        encoder_input = x[\"encoder_input\"] # (b, seq_len)\n",
    "        encoder_mask = x[\"encoder_mask\"] # (b, 1, 1, seq_len) \n",
    "        model_out = greedy_decode_pl(self.model, encoder_input, encoder_mask, self.tokenizer_src, self.tokenizer_tgt, self.max_len)\n",
    "        source_text = x[\"src_text\"][0]\n",
    "        target_text = x[\"tgt_text\"][0] \n",
    "        model_out_text = self.tokenizer_tgt.decode(model_out.detach().cpu().numpy()) \n",
    "\n",
    "        print(f'SOURCE: {source_text}')   \n",
    "        print(f'TARGET: {target_text}')\n",
    "        print(f'PREDICTED (Lavanya Nemani & Shashank Gupta): {model_out_text}')\n",
    "        return None\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config['lr'], eps=1e-9)\n",
    "        return {\"optimizer\": optimizer}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        proj_output = self.forward(x)\n",
    "        label = x['label']\n",
    "        loss = self.loss_fn(proj_output.view(-1, self.tokenizer_tgt.get_vocab_size()), label.view(-1)) \n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch_idx == 0: \n",
    "            x = batch\n",
    "            self.forward_inference(x)\n",
    "        return None \n",
    "    \n",
    "    ## TODO: print only 1 random example from val_dataloader\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     val_loader = self.val_dataloader()\n",
    "    #     random_batch_index = random.randint(0, len(val_loader))\n",
    "    #     for batch_index, batch in enumerate(val_loader):\n",
    "    #         if batch_index != random_batch_index:\n",
    "    #             continue\n",
    "    #         x = batch\n",
    "    #         self.forward_inference(x)\n",
    "    #     return \n",
    "    \n",
    "    def get_dataloaders(self):\n",
    "        train_loader, val_loader, _, _ = get_ds(self.config)\n",
    "        return train_loader, val_loader \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader, _ = self.get_dataloaders()\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        _, val_loader = self.get_dataloaders()\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs (Lavanya Nemani & Shashank Gupta)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "    | Name                                                  | Type                    | Params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0   | model                                                 | Transformer             | 75.1 M\n",
      "1   | model.encoder                                         | Encoder                 | 18.9 M\n",
      "2   | model.encoder.layers                                  | ModuleList              | 18.9 M\n",
      "3   | model.encoder.layers.0                                | EncoderBlock            | 3.1 M \n",
      "4   | model.encoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "5   | model.encoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "6   | model.encoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "7   | model.encoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "8   | model.encoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "9   | model.encoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "10  | model.encoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "11  | model.encoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "12  | model.encoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "13  | model.encoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "14  | model.encoder.layers.0.residual_connections           | ModuleList              | 4     \n",
      "15  | model.encoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "16  | model.encoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "17  | model.encoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "18  | model.encoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "19  | model.encoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "20  | model.encoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "21  | model.encoder.layers.1                                | EncoderBlock            | 3.1 M \n",
      "22  | model.encoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "23  | model.encoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "24  | model.encoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "25  | model.encoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "26  | model.encoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "27  | model.encoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "28  | model.encoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "29  | model.encoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "30  | model.encoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "31  | model.encoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "32  | model.encoder.layers.1.residual_connections           | ModuleList              | 4     \n",
      "33  | model.encoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "34  | model.encoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "35  | model.encoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "36  | model.encoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "37  | model.encoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "38  | model.encoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "39  | model.encoder.layers.2                                | EncoderBlock            | 3.1 M \n",
      "40  | model.encoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "41  | model.encoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "42  | model.encoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "43  | model.encoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "44  | model.encoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "45  | model.encoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "46  | model.encoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "47  | model.encoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "48  | model.encoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "49  | model.encoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "50  | model.encoder.layers.2.residual_connections           | ModuleList              | 4     \n",
      "51  | model.encoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "52  | model.encoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "53  | model.encoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "54  | model.encoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "55  | model.encoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "56  | model.encoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "57  | model.encoder.layers.3                                | EncoderBlock            | 3.1 M \n",
      "58  | model.encoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "59  | model.encoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n",
      "60  | model.encoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n",
      "61  | model.encoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n",
      "62  | model.encoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n",
      "63  | model.encoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n",
      "64  | model.encoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "65  | model.encoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "66  | model.encoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "67  | model.encoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "68  | model.encoder.layers.3.residual_connections           | ModuleList              | 4     \n",
      "69  | model.encoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n",
      "70  | model.encoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n",
      "71  | model.encoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "72  | model.encoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n",
      "73  | model.encoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n",
      "74  | model.encoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "75  | model.encoder.layers.4                                | EncoderBlock            | 3.1 M \n",
      "76  | model.encoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "77  | model.encoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n",
      "78  | model.encoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n",
      "79  | model.encoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n",
      "80  | model.encoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n",
      "81  | model.encoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n",
      "82  | model.encoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "83  | model.encoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "84  | model.encoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "85  | model.encoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "86  | model.encoder.layers.4.residual_connections           | ModuleList              | 4     \n",
      "87  | model.encoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n",
      "88  | model.encoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n",
      "89  | model.encoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "90  | model.encoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n",
      "91  | model.encoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n",
      "92  | model.encoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "93  | model.encoder.layers.5                                | EncoderBlock            | 3.1 M \n",
      "94  | model.encoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "95  | model.encoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n",
      "96  | model.encoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n",
      "97  | model.encoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n",
      "98  | model.encoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n",
      "99  | model.encoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n",
      "100 | model.encoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "101 | model.encoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "102 | model.encoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "103 | model.encoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "104 | model.encoder.layers.5.residual_connections           | ModuleList              | 4     \n",
      "105 | model.encoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n",
      "106 | model.encoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n",
      "107 | model.encoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "108 | model.encoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n",
      "109 | model.encoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n",
      "110 | model.encoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "111 | model.encoder.norm                                    | LayerNormalization      | 2     \n",
      "112 | model.decoder                                         | Decoder                 | 25.2 M\n",
      "113 | model.decoder.layers                                  | ModuleList              | 25.2 M\n",
      "114 | model.decoder.layers.0                                | DecoderBlock            | 4.2 M \n",
      "115 | model.decoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "116 | model.decoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "117 | model.decoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "118 | model.decoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "119 | model.decoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "120 | model.decoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "121 | model.decoder.layers.0.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "122 | model.decoder.layers.0.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "123 | model.decoder.layers.0.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "124 | model.decoder.layers.0.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "125 | model.decoder.layers.0.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "126 | model.decoder.layers.0.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "127 | model.decoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "128 | model.decoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "129 | model.decoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "130 | model.decoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "131 | model.decoder.layers.0.residual_connections           | ModuleList              | 6     \n",
      "132 | model.decoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "133 | model.decoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "134 | model.decoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "135 | model.decoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "136 | model.decoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "137 | model.decoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "138 | model.decoder.layers.0.residual_connections.2         | ResidualConnection      | 2     \n",
      "139 | model.decoder.layers.0.residual_connections.2.dropout | Dropout                 | 0     \n",
      "140 | model.decoder.layers.0.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "141 | model.decoder.layers.1                                | DecoderBlock            | 4.2 M \n",
      "142 | model.decoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "143 | model.decoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "144 | model.decoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "145 | model.decoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "146 | model.decoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "147 | model.decoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "148 | model.decoder.layers.1.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "149 | model.decoder.layers.1.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "150 | model.decoder.layers.1.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "151 | model.decoder.layers.1.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "152 | model.decoder.layers.1.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "153 | model.decoder.layers.1.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "154 | model.decoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "155 | model.decoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "156 | model.decoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "157 | model.decoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "158 | model.decoder.layers.1.residual_connections           | ModuleList              | 6     \n",
      "159 | model.decoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "160 | model.decoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "161 | model.decoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "162 | model.decoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "163 | model.decoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "164 | model.decoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "165 | model.decoder.layers.1.residual_connections.2         | ResidualConnection      | 2     \n",
      "166 | model.decoder.layers.1.residual_connections.2.dropout | Dropout                 | 0     \n",
      "167 | model.decoder.layers.1.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "168 | model.decoder.layers.2                                | DecoderBlock            | 4.2 M \n",
      "169 | model.decoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "170 | model.decoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "171 | model.decoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "172 | model.decoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "173 | model.decoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "174 | model.decoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "175 | model.decoder.layers.2.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "176 | model.decoder.layers.2.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "177 | model.decoder.layers.2.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "178 | model.decoder.layers.2.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "179 | model.decoder.layers.2.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "180 | model.decoder.layers.2.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "181 | model.decoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "182 | model.decoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "183 | model.decoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "184 | model.decoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "185 | model.decoder.layers.2.residual_connections           | ModuleList              | 6     \n",
      "186 | model.decoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "187 | model.decoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "188 | model.decoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "189 | model.decoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "190 | model.decoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "191 | model.decoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "192 | model.decoder.layers.2.residual_connections.2         | ResidualConnection      | 2     \n",
      "193 | model.decoder.layers.2.residual_connections.2.dropout | Dropout                 | 0     \n",
      "194 | model.decoder.layers.2.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "195 | model.decoder.layers.3                                | DecoderBlock            | 4.2 M \n",
      "196 | model.decoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "197 | model.decoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n",
      "198 | model.decoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n",
      "199 | model.decoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n",
      "200 | model.decoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n",
      "201 | model.decoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n",
      "202 | model.decoder.layers.3.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "203 | model.decoder.layers.3.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "204 | model.decoder.layers.3.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "205 | model.decoder.layers.3.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "206 | model.decoder.layers.3.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "207 | model.decoder.layers.3.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "208 | model.decoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "209 | model.decoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "210 | model.decoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "211 | model.decoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "212 | model.decoder.layers.3.residual_connections           | ModuleList              | 6     \n",
      "213 | model.decoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n",
      "214 | model.decoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n",
      "215 | model.decoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "216 | model.decoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n",
      "217 | model.decoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n",
      "218 | model.decoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "219 | model.decoder.layers.3.residual_connections.2         | ResidualConnection      | 2     \n",
      "220 | model.decoder.layers.3.residual_connections.2.dropout | Dropout                 | 0     \n",
      "221 | model.decoder.layers.3.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "222 | model.decoder.layers.4                                | DecoderBlock            | 4.2 M \n",
      "223 | model.decoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "224 | model.decoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n",
      "225 | model.decoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n",
      "226 | model.decoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n",
      "227 | model.decoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n",
      "228 | model.decoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n",
      "229 | model.decoder.layers.4.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "230 | model.decoder.layers.4.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "231 | model.decoder.layers.4.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "232 | model.decoder.layers.4.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "233 | model.decoder.layers.4.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "234 | model.decoder.layers.4.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "235 | model.decoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "236 | model.decoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "237 | model.decoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "238 | model.decoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "239 | model.decoder.layers.4.residual_connections           | ModuleList              | 6     \n",
      "240 | model.decoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n",
      "241 | model.decoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n",
      "242 | model.decoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "243 | model.decoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n",
      "244 | model.decoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n",
      "245 | model.decoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "246 | model.decoder.layers.4.residual_connections.2         | ResidualConnection      | 2     \n",
      "247 | model.decoder.layers.4.residual_connections.2.dropout | Dropout                 | 0     \n",
      "248 | model.decoder.layers.4.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "249 | model.decoder.layers.5                                | DecoderBlock            | 4.2 M \n",
      "250 | model.decoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "251 | model.decoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n",
      "252 | model.decoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n",
      "253 | model.decoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n",
      "254 | model.decoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n",
      "255 | model.decoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n",
      "256 | model.decoder.layers.5.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "257 | model.decoder.layers.5.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "258 | model.decoder.layers.5.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "259 | model.decoder.layers.5.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "260 | model.decoder.layers.5.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "261 | model.decoder.layers.5.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "262 | model.decoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "263 | model.decoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "264 | model.decoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "265 | model.decoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "266 | model.decoder.layers.5.residual_connections           | ModuleList              | 6     \n",
      "267 | model.decoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n",
      "268 | model.decoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n",
      "269 | model.decoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "270 | model.decoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n",
      "271 | model.decoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n",
      "272 | model.decoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "273 | model.decoder.layers.5.residual_connections.2         | ResidualConnection      | 2     \n",
      "274 | model.decoder.layers.5.residual_connections.2.dropout | Dropout                 | 0     \n",
      "275 | model.decoder.layers.5.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "276 | model.decoder.norm                                    | LayerNormalization      | 2     \n",
      "277 | model.src_embed                                       | InputEmbeddings         | 8.0 M \n",
      "278 | model.src_embed.embedding                             | Embedding               | 8.0 M \n",
      "279 | model.tgt_embed                                       | InputEmbeddings         | 11.5 M\n",
      "280 | model.tgt_embed.embedding                             | Embedding               | 11.5 M\n",
      "281 | model.src_pos                                         | PositionalEncoding      | 0     \n",
      "282 | model.src_pos.dropout                                 | Dropout                 | 0     \n",
      "283 | model.tgt_pos                                         | PositionalEncoding      | 0     \n",
      "284 | model.tgt_pos.dropout                                 | Dropout                 | 0     \n",
      "285 | model.projection_layer                                | ProjectionLayer         | 11.5 M\n",
      "286 | model.projection_layer.proj                           | Linear                  | 11.5 M\n",
      "287 | loss_fn                                               | CrossEntropyLoss        | 0     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "75.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "75.1 M    Total params\n",
      "300.532   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac9b1718d7e4b219784a4db3a89d10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of source sentence: 309\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of source sentence: 309\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of target sentence: 274\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of target sentence: 274\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:486: \n",
       "PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you \n",
       "turn shuffling off for val/test dataloaders.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:486: \n",
       "PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you \n",
       "turn shuffling off for val/test dataloaders.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: \n",
       "PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the \n",
       "`DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: \n",
       "PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider\n",
       "increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the \n",
       "`DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: So they sat down, and nobody spoke for some minutes.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: So they sat down, and nobody spoke for some minutes.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: E sedettero e per qualche minuto nessuno parlò.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: E sedettero e per qualche minuto nessuno parlò.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): occupazione occupazione occupazione occupazione occupazione leggerli \n",
       "congiunse congiunse sull occupazione congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse comprò baroni congiunse moriranno congiunse congiunse congiunse \n",
       "congiunse moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno \n",
       "moriranno congiunse congiunse congiunse congiunse congiunse moriranno congiunse moriranno occupazione congiunse \n",
       "moriranno occupazione congiunse congiunse congiunse congiunse occupazione occupazione occupazione congiunse \n",
       "congiunse congiunse occupazione occupazione occupazione congiunse congiunse congiunse congiunse congiunse congiunse\n",
       "congiunse congiunse ondeggiante congiunse congiunse congiunse congiunse occupazione occupazione occupazione \n",
       "occupazione congiunse congiunse congiunse congiunse congiunse occupazione occupazione occupazione occupazione \n",
       "occupazione occupazione moriranno occupazione congiunse congiunse congiunse occupazione occupazione occupazione \n",
       "allora allora allora allora inattesa inattesa occupazione occupazione occupazione occupazione inattesa inattesa \n",
       "occupazione congiunse congiunse congiunse moriranno occupazione inattesa inattesa inattesa occupazione occupazione \n",
       "occupazione inattesa congiunse congiunse congiunse congiunse congiunse congiunse congiunse moriranno congiunse \n",
       "congiunse moriranno capitata capitata capitata capitata occupazione occupazione occupazione occupazione capitata \n",
       "capitata capitata capitata capitata moriranno inattesa congiunse congiunse congiunse occupazione allora allora \n",
       "salato salato allora allora allora inattesa occupazione inattesa occupazione inattesa occupazione occupazione \n",
       "occupazione passeggiero congiunse congiunse congiunse occupazione occupazione occupazione passeggiero congiunse \n",
       "occupazione occupazione occupazione occupazione congiunse congiunse congiunse congiunse congiunse occupazione \n",
       "moriranno occupazione congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse passeggiero comprò passeggiero comprò passeggiero comprò allora allora \n",
       "comprò comprò comprò comprò passeggiero congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse passeggiero comprò comprò comprò comprò comprò congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse inattesa inattesa inattesa inattesa passeggiero passeggiero comprò passeggiero comprò comprò \n",
       "passeggiero passeggiero passeggiero passeggiero passeggiero passeggiero passeggiero comprò passeggiero comprò \n",
       "passeggiero\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): occupazione occupazione occupazione occupazione occupazione leggerli \n",
       "congiunse congiunse sull occupazione congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse comprò baroni congiunse moriranno congiunse congiunse congiunse \n",
       "congiunse moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno moriranno \n",
       "moriranno congiunse congiunse congiunse congiunse congiunse moriranno congiunse moriranno occupazione congiunse \n",
       "moriranno occupazione congiunse congiunse congiunse congiunse occupazione occupazione occupazione congiunse \n",
       "congiunse congiunse occupazione occupazione occupazione congiunse congiunse congiunse congiunse congiunse congiunse\n",
       "congiunse congiunse ondeggiante congiunse congiunse congiunse congiunse occupazione occupazione occupazione \n",
       "occupazione congiunse congiunse congiunse congiunse congiunse occupazione occupazione occupazione occupazione \n",
       "occupazione occupazione moriranno occupazione congiunse congiunse congiunse occupazione occupazione occupazione \n",
       "allora allora allora allora inattesa inattesa occupazione occupazione occupazione occupazione inattesa inattesa \n",
       "occupazione congiunse congiunse congiunse moriranno occupazione inattesa inattesa inattesa occupazione occupazione \n",
       "occupazione inattesa congiunse congiunse congiunse congiunse congiunse congiunse congiunse moriranno congiunse \n",
       "congiunse moriranno capitata capitata capitata capitata occupazione occupazione occupazione occupazione capitata \n",
       "capitata capitata capitata capitata moriranno inattesa congiunse congiunse congiunse occupazione allora allora \n",
       "salato salato allora allora allora inattesa occupazione inattesa occupazione inattesa occupazione occupazione \n",
       "occupazione passeggiero congiunse congiunse congiunse occupazione occupazione occupazione passeggiero congiunse \n",
       "occupazione occupazione occupazione occupazione congiunse congiunse congiunse congiunse congiunse occupazione \n",
       "moriranno occupazione congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse congiunse congiunse passeggiero comprò passeggiero comprò passeggiero comprò allora allora \n",
       "comprò comprò comprò comprò passeggiero congiunse congiunse congiunse congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse passeggiero comprò comprò comprò comprò comprò congiunse congiunse congiunse congiunse \n",
       "congiunse congiunse inattesa inattesa inattesa inattesa passeggiero passeggiero comprò passeggiero comprò comprò \n",
       "passeggiero passeggiero passeggiero passeggiero passeggiero passeggiero passeggiero comprò passeggiero comprò \n",
       "passeggiero\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of source sentence: 309\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of source sentence: 309\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of target sentence: 274\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of target sentence: 274\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: \n",
       "PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) \n",
       "in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/user/miniconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: \n",
       "PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. \n",
       "Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) \n",
       "in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: 'Oh, you know!'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: 'Oh, you know!'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: — Ah, lo sapete!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: — Ah, lo sapete!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): — Non è ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): — Non è ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191ea2333754de9848242ea2fa9e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: I leave both the choice of subject and the manner of treating it entirely to yourself.\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: I leave both the choice of subject and the manner of treating it entirely to yourself.\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: — Di quello che vi pare; vi lascio la scelta dell'argomento e potrete trattarlo come vi aggrada.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: — Di quello che vi pare; vi lascio la scelta dell'argomento e potrete trattarlo come vi aggrada.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): la mia sua e la mia sua vita .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): la mia sua e la mia sua vita .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b07724503e943be91bdd1362bcc047e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: I will go back as soon as I can stir: I need not make an absolute fool of myself.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: I will go back as soon as I can stir: I need not make an absolute fool of myself.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Volevo tornare addietro, appena avessi potuto; non volevo sembrare pazza.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Volevo tornare addietro, appena avessi potuto; non volevo sembrare pazza.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Mi a me , ma non mi , ma non mi .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Mi a me , ma non mi , ma non mi .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e027f1da0efc4a56a2c30e6dfa400c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: Besides, it's impossible to feel awkward with him.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: Besides, it's impossible to feel awkward with him.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: E poi con lui non ci si può sentire impacciati.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: E poi con lui non ci si può sentire impacciati.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Ma , non si può essere più più di lui , e Levin .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Ma , non si può essere più più di lui , e Levin .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a26d58c0bf42b9b4e54fee9a14bfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: Something magical has happened to me: like a dream when one feels frightened and creepy, and suddenly wakes\n",
       "up to the knowledge that no such terrors exist.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: Something magical has happened to me: like a dream when one feels frightened and creepy, and suddenly wakes\n",
       "up to the knowledge that no such terrors exist.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: M’è accaduto qualcosa di magico, come quando in un sogno si prova spavento, impressione e a un tratto ci si\n",
       "sveglia e si sente che tutti quegli spaventi non esistono.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: M’è accaduto qualcosa di magico, come quando in un sogno si prova spavento, impressione e a un tratto ci si\n",
       "sveglia e si sente che tutti quegli spaventi non esistono.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): È un ' altra cosa , che mi ha detto , quando un ' altra , e che , come\n",
       "un uomo , e che non si .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): È un ' altra cosa , che mi ha detto , quando un ' altra , e che , come\n",
       "un uomo , e che non si .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36d0b924a3c431bacd4bc5662682da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: 'I do not wish to offend,' his every look seemed to say, 'I only wish to save myself, but I do not know \n",
       "how.'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: 'I do not wish to offend,' his every look seemed to say, 'I only wish to save myself, but I do not know \n",
       "how.'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: «Non voglio offendervi — diceva ogni volta il suo sguardo — ma voglio salvarmi e non so come».\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: «Non voglio offendervi — diceva ogni volta il suo sguardo — ma voglio salvarmi e non so come».\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): — Non posso dire — disse , — che gli occhi si di nuovo , non posso \n",
       "dire che io non posso dire .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): — Non posso dire — disse , — che gli occhi si di nuovo , non posso \n",
       "dire che io non posso dire .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3a15da7ab54ed790ff4404b71ae833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: Diana and Mary Rivers became more sad and silent as the day approached for leaving their brother and their \n",
       "home.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: Diana and Mary Rivers became more sad and silent as the day approached for leaving their brother and their \n",
       "home.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Maria e Diana si facevano più tristi quanto più avvicinavasi il momento di lasciar la casa e il fratello.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Maria e Diana si facevano più tristi quanto più avvicinavasi il momento di lasciar la casa e il fratello.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Diana e Maria si alzò e si alzò per la casa , e la mattina si mise a \n",
       "casa .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Diana e Maria si alzò e si alzò per la casa , e la mattina si mise a \n",
       "casa .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d729de98ec64fef8589481a4286ac88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: His methodical mind had formed definite views on the life of the people, founded partly on that life \n",
       "itself, but chiefly on its contrast.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: His methodical mind had formed definite views on the life of the people, founded partly on that life \n",
       "itself, but chiefly on its contrast.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Nella sua mente ordinata si erano chiaramente fissate le forme definite della vita rurale, tratte, in \n",
       "parte, dalla stessa vita del contadino, ma in prevalenza da quella contrapposizione.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Nella sua mente ordinata si erano chiaramente fissate le forme definite della vita rurale, tratte, in \n",
       "parte, dalla stessa vita del contadino, ma in prevalenza da quella contrapposizione.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Il suo pensiero aveva l ’ idea di dei rapporti della vita , di , di \n",
       "essere in modo , ma in modo di essere sempre più forte .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Il suo pensiero aveva l ’ idea di dei rapporti della vita , di , di \n",
       "essere in modo , ma in modo di essere sempre più forte .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5029eb14bf834a88a126adcba2a374b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: In doing so I will keep to the order indicated above, and discuss how such principalities are to be ruled \n",
       "and preserved.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: In doing so I will keep to the order indicated above, and discuss how such principalities are to be ruled \n",
       "and preserved.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Volterommi solo al principato, et andrò tessendo li orditi soprascritti, e disputerò come questi principati\n",
       "si possino governare e mantenere.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Volterommi solo al principato, et andrò tessendo li orditi soprascritti, e disputerò come questi principati\n",
       "si possino governare e mantenere.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): a , e le cose che sono stati , e come sono stati , e quando sono stati\n",
       "e .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): a , e le cose che sono stati , e come sono stati , e quando sono stati\n",
       "e .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139227cb6da747fa910c358d34f3d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: For one moment of that bliss...'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: For one moment of that bliss...'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Per me, un attimo di questa felicità....\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Per me, un attimo di questa felicità....\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Per un attimo è la felicità ...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Per un attimo è la felicità ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = Lit_Transformer()\n",
    "\n",
    "print('Logs (Lavanya Nemani & Shashank Gupta)')\n",
    "\n",
    "## TODO: implement multi-gpu training\n",
    "trainer = Trainer(\n",
    "    callbacks=[ModelSummary(max_depth=-1), RichProgressBar(leave=True)],\n",
    "    accelerator=\"gpu\", devices=[1],\n",
    "    max_epochs = epochs,\n",
    "    )\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "    | Name                                                  | Type                    | Params\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0   | model                                                 | Transformer             | 75.1 M\n",
      "1   | model.encoder                                         | Encoder                 | 18.9 M\n",
      "2   | model.encoder.layers                                  | ModuleList              | 18.9 M\n",
      "3   | model.encoder.layers.0                                | EncoderBlock            | 3.1 M \n",
      "4   | model.encoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "5   | model.encoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "6   | model.encoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "7   | model.encoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "8   | model.encoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "9   | model.encoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "10  | model.encoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "11  | model.encoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "12  | model.encoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "13  | model.encoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "14  | model.encoder.layers.0.residual_connections           | ModuleList              | 4     \n",
      "15  | model.encoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "16  | model.encoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "17  | model.encoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "18  | model.encoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "19  | model.encoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "20  | model.encoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "21  | model.encoder.layers.1                                | EncoderBlock            | 3.1 M \n",
      "22  | model.encoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "23  | model.encoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "24  | model.encoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "25  | model.encoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "26  | model.encoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "27  | model.encoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "28  | model.encoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "29  | model.encoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "30  | model.encoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "31  | model.encoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "32  | model.encoder.layers.1.residual_connections           | ModuleList              | 4     \n",
      "33  | model.encoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "34  | model.encoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "35  | model.encoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "36  | model.encoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "37  | model.encoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "38  | model.encoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "39  | model.encoder.layers.2                                | EncoderBlock            | 3.1 M \n",
      "40  | model.encoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "41  | model.encoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "42  | model.encoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "43  | model.encoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "44  | model.encoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "45  | model.encoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "46  | model.encoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "47  | model.encoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "48  | model.encoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "49  | model.encoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "50  | model.encoder.layers.2.residual_connections           | ModuleList              | 4     \n",
      "51  | model.encoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "52  | model.encoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "53  | model.encoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "54  | model.encoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "55  | model.encoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "56  | model.encoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "57  | model.encoder.layers.3                                | EncoderBlock            | 3.1 M \n",
      "58  | model.encoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "59  | model.encoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n",
      "60  | model.encoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n",
      "61  | model.encoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n",
      "62  | model.encoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n",
      "63  | model.encoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n",
      "64  | model.encoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "65  | model.encoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "66  | model.encoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "67  | model.encoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "68  | model.encoder.layers.3.residual_connections           | ModuleList              | 4     \n",
      "69  | model.encoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n",
      "70  | model.encoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n",
      "71  | model.encoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "72  | model.encoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n",
      "73  | model.encoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n",
      "74  | model.encoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "75  | model.encoder.layers.4                                | EncoderBlock            | 3.1 M \n",
      "76  | model.encoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "77  | model.encoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n",
      "78  | model.encoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n",
      "79  | model.encoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n",
      "80  | model.encoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n",
      "81  | model.encoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n",
      "82  | model.encoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "83  | model.encoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "84  | model.encoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "85  | model.encoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "86  | model.encoder.layers.4.residual_connections           | ModuleList              | 4     \n",
      "87  | model.encoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n",
      "88  | model.encoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n",
      "89  | model.encoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "90  | model.encoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n",
      "91  | model.encoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n",
      "92  | model.encoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "93  | model.encoder.layers.5                                | EncoderBlock            | 3.1 M \n",
      "94  | model.encoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "95  | model.encoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n",
      "96  | model.encoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n",
      "97  | model.encoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n",
      "98  | model.encoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n",
      "99  | model.encoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n",
      "100 | model.encoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "101 | model.encoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "102 | model.encoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "103 | model.encoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "104 | model.encoder.layers.5.residual_connections           | ModuleList              | 4     \n",
      "105 | model.encoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n",
      "106 | model.encoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n",
      "107 | model.encoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "108 | model.encoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n",
      "109 | model.encoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n",
      "110 | model.encoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "111 | model.encoder.norm                                    | LayerNormalization      | 2     \n",
      "112 | model.decoder                                         | Decoder                 | 25.2 M\n",
      "113 | model.decoder.layers                                  | ModuleList              | 25.2 M\n",
      "114 | model.decoder.layers.0                                | DecoderBlock            | 4.2 M \n",
      "115 | model.decoder.layers.0.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "116 | model.decoder.layers.0.self_attention_block.w_q       | Linear                  | 262 K \n",
      "117 | model.decoder.layers.0.self_attention_block.w_k       | Linear                  | 262 K \n",
      "118 | model.decoder.layers.0.self_attention_block.w_v       | Linear                  | 262 K \n",
      "119 | model.decoder.layers.0.self_attention_block.w_o       | Linear                  | 262 K \n",
      "120 | model.decoder.layers.0.self_attention_block.dropout   | Dropout                 | 0     \n",
      "121 | model.decoder.layers.0.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "122 | model.decoder.layers.0.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "123 | model.decoder.layers.0.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "124 | model.decoder.layers.0.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "125 | model.decoder.layers.0.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "126 | model.decoder.layers.0.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "127 | model.decoder.layers.0.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "128 | model.decoder.layers.0.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "129 | model.decoder.layers.0.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "130 | model.decoder.layers.0.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "131 | model.decoder.layers.0.residual_connections           | ModuleList              | 6     \n",
      "132 | model.decoder.layers.0.residual_connections.0         | ResidualConnection      | 2     \n",
      "133 | model.decoder.layers.0.residual_connections.0.dropout | Dropout                 | 0     \n",
      "134 | model.decoder.layers.0.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "135 | model.decoder.layers.0.residual_connections.1         | ResidualConnection      | 2     \n",
      "136 | model.decoder.layers.0.residual_connections.1.dropout | Dropout                 | 0     \n",
      "137 | model.decoder.layers.0.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "138 | model.decoder.layers.0.residual_connections.2         | ResidualConnection      | 2     \n",
      "139 | model.decoder.layers.0.residual_connections.2.dropout | Dropout                 | 0     \n",
      "140 | model.decoder.layers.0.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "141 | model.decoder.layers.1                                | DecoderBlock            | 4.2 M \n",
      "142 | model.decoder.layers.1.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "143 | model.decoder.layers.1.self_attention_block.w_q       | Linear                  | 262 K \n",
      "144 | model.decoder.layers.1.self_attention_block.w_k       | Linear                  | 262 K \n",
      "145 | model.decoder.layers.1.self_attention_block.w_v       | Linear                  | 262 K \n",
      "146 | model.decoder.layers.1.self_attention_block.w_o       | Linear                  | 262 K \n",
      "147 | model.decoder.layers.1.self_attention_block.dropout   | Dropout                 | 0     \n",
      "148 | model.decoder.layers.1.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "149 | model.decoder.layers.1.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "150 | model.decoder.layers.1.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "151 | model.decoder.layers.1.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "152 | model.decoder.layers.1.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "153 | model.decoder.layers.1.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "154 | model.decoder.layers.1.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "155 | model.decoder.layers.1.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "156 | model.decoder.layers.1.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "157 | model.decoder.layers.1.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "158 | model.decoder.layers.1.residual_connections           | ModuleList              | 6     \n",
      "159 | model.decoder.layers.1.residual_connections.0         | ResidualConnection      | 2     \n",
      "160 | model.decoder.layers.1.residual_connections.0.dropout | Dropout                 | 0     \n",
      "161 | model.decoder.layers.1.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "162 | model.decoder.layers.1.residual_connections.1         | ResidualConnection      | 2     \n",
      "163 | model.decoder.layers.1.residual_connections.1.dropout | Dropout                 | 0     \n",
      "164 | model.decoder.layers.1.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "165 | model.decoder.layers.1.residual_connections.2         | ResidualConnection      | 2     \n",
      "166 | model.decoder.layers.1.residual_connections.2.dropout | Dropout                 | 0     \n",
      "167 | model.decoder.layers.1.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "168 | model.decoder.layers.2                                | DecoderBlock            | 4.2 M \n",
      "169 | model.decoder.layers.2.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "170 | model.decoder.layers.2.self_attention_block.w_q       | Linear                  | 262 K \n",
      "171 | model.decoder.layers.2.self_attention_block.w_k       | Linear                  | 262 K \n",
      "172 | model.decoder.layers.2.self_attention_block.w_v       | Linear                  | 262 K \n",
      "173 | model.decoder.layers.2.self_attention_block.w_o       | Linear                  | 262 K \n",
      "174 | model.decoder.layers.2.self_attention_block.dropout   | Dropout                 | 0     \n",
      "175 | model.decoder.layers.2.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "176 | model.decoder.layers.2.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "177 | model.decoder.layers.2.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "178 | model.decoder.layers.2.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "179 | model.decoder.layers.2.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "180 | model.decoder.layers.2.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "181 | model.decoder.layers.2.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "182 | model.decoder.layers.2.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "183 | model.decoder.layers.2.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "184 | model.decoder.layers.2.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "185 | model.decoder.layers.2.residual_connections           | ModuleList              | 6     \n",
      "186 | model.decoder.layers.2.residual_connections.0         | ResidualConnection      | 2     \n",
      "187 | model.decoder.layers.2.residual_connections.0.dropout | Dropout                 | 0     \n",
      "188 | model.decoder.layers.2.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "189 | model.decoder.layers.2.residual_connections.1         | ResidualConnection      | 2     \n",
      "190 | model.decoder.layers.2.residual_connections.1.dropout | Dropout                 | 0     \n",
      "191 | model.decoder.layers.2.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "192 | model.decoder.layers.2.residual_connections.2         | ResidualConnection      | 2     \n",
      "193 | model.decoder.layers.2.residual_connections.2.dropout | Dropout                 | 0     \n",
      "194 | model.decoder.layers.2.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "195 | model.decoder.layers.3                                | DecoderBlock            | 4.2 M \n",
      "196 | model.decoder.layers.3.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "197 | model.decoder.layers.3.self_attention_block.w_q       | Linear                  | 262 K \n",
      "198 | model.decoder.layers.3.self_attention_block.w_k       | Linear                  | 262 K \n",
      "199 | model.decoder.layers.3.self_attention_block.w_v       | Linear                  | 262 K \n",
      "200 | model.decoder.layers.3.self_attention_block.w_o       | Linear                  | 262 K \n",
      "201 | model.decoder.layers.3.self_attention_block.dropout   | Dropout                 | 0     \n",
      "202 | model.decoder.layers.3.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "203 | model.decoder.layers.3.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "204 | model.decoder.layers.3.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "205 | model.decoder.layers.3.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "206 | model.decoder.layers.3.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "207 | model.decoder.layers.3.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "208 | model.decoder.layers.3.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "209 | model.decoder.layers.3.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "210 | model.decoder.layers.3.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "211 | model.decoder.layers.3.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "212 | model.decoder.layers.3.residual_connections           | ModuleList              | 6     \n",
      "213 | model.decoder.layers.3.residual_connections.0         | ResidualConnection      | 2     \n",
      "214 | model.decoder.layers.3.residual_connections.0.dropout | Dropout                 | 0     \n",
      "215 | model.decoder.layers.3.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "216 | model.decoder.layers.3.residual_connections.1         | ResidualConnection      | 2     \n",
      "217 | model.decoder.layers.3.residual_connections.1.dropout | Dropout                 | 0     \n",
      "218 | model.decoder.layers.3.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "219 | model.decoder.layers.3.residual_connections.2         | ResidualConnection      | 2     \n",
      "220 | model.decoder.layers.3.residual_connections.2.dropout | Dropout                 | 0     \n",
      "221 | model.decoder.layers.3.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "222 | model.decoder.layers.4                                | DecoderBlock            | 4.2 M \n",
      "223 | model.decoder.layers.4.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "224 | model.decoder.layers.4.self_attention_block.w_q       | Linear                  | 262 K \n",
      "225 | model.decoder.layers.4.self_attention_block.w_k       | Linear                  | 262 K \n",
      "226 | model.decoder.layers.4.self_attention_block.w_v       | Linear                  | 262 K \n",
      "227 | model.decoder.layers.4.self_attention_block.w_o       | Linear                  | 262 K \n",
      "228 | model.decoder.layers.4.self_attention_block.dropout   | Dropout                 | 0     \n",
      "229 | model.decoder.layers.4.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "230 | model.decoder.layers.4.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "231 | model.decoder.layers.4.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "232 | model.decoder.layers.4.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "233 | model.decoder.layers.4.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "234 | model.decoder.layers.4.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "235 | model.decoder.layers.4.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "236 | model.decoder.layers.4.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "237 | model.decoder.layers.4.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "238 | model.decoder.layers.4.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "239 | model.decoder.layers.4.residual_connections           | ModuleList              | 6     \n",
      "240 | model.decoder.layers.4.residual_connections.0         | ResidualConnection      | 2     \n",
      "241 | model.decoder.layers.4.residual_connections.0.dropout | Dropout                 | 0     \n",
      "242 | model.decoder.layers.4.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "243 | model.decoder.layers.4.residual_connections.1         | ResidualConnection      | 2     \n",
      "244 | model.decoder.layers.4.residual_connections.1.dropout | Dropout                 | 0     \n",
      "245 | model.decoder.layers.4.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "246 | model.decoder.layers.4.residual_connections.2         | ResidualConnection      | 2     \n",
      "247 | model.decoder.layers.4.residual_connections.2.dropout | Dropout                 | 0     \n",
      "248 | model.decoder.layers.4.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "249 | model.decoder.layers.5                                | DecoderBlock            | 4.2 M \n",
      "250 | model.decoder.layers.5.self_attention_block           | MultiHeadAttentionBlock | 1.0 M \n",
      "251 | model.decoder.layers.5.self_attention_block.w_q       | Linear                  | 262 K \n",
      "252 | model.decoder.layers.5.self_attention_block.w_k       | Linear                  | 262 K \n",
      "253 | model.decoder.layers.5.self_attention_block.w_v       | Linear                  | 262 K \n",
      "254 | model.decoder.layers.5.self_attention_block.w_o       | Linear                  | 262 K \n",
      "255 | model.decoder.layers.5.self_attention_block.dropout   | Dropout                 | 0     \n",
      "256 | model.decoder.layers.5.cross_attention_block          | MultiHeadAttentionBlock | 1.0 M \n",
      "257 | model.decoder.layers.5.cross_attention_block.w_q      | Linear                  | 262 K \n",
      "258 | model.decoder.layers.5.cross_attention_block.w_k      | Linear                  | 262 K \n",
      "259 | model.decoder.layers.5.cross_attention_block.w_v      | Linear                  | 262 K \n",
      "260 | model.decoder.layers.5.cross_attention_block.w_o      | Linear                  | 262 K \n",
      "261 | model.decoder.layers.5.cross_attention_block.dropout  | Dropout                 | 0     \n",
      "262 | model.decoder.layers.5.feed_forward_block             | FeedForwardBlock        | 2.1 M \n",
      "263 | model.decoder.layers.5.feed_forward_block.linear_1    | Linear                  | 1.1 M \n",
      "264 | model.decoder.layers.5.feed_forward_block.dropout     | Dropout                 | 0     \n",
      "265 | model.decoder.layers.5.feed_forward_block.linear_2    | Linear                  | 1.0 M \n",
      "266 | model.decoder.layers.5.residual_connections           | ModuleList              | 6     \n",
      "267 | model.decoder.layers.5.residual_connections.0         | ResidualConnection      | 2     \n",
      "268 | model.decoder.layers.5.residual_connections.0.dropout | Dropout                 | 0     \n",
      "269 | model.decoder.layers.5.residual_connections.0.norm    | LayerNormalization      | 2     \n",
      "270 | model.decoder.layers.5.residual_connections.1         | ResidualConnection      | 2     \n",
      "271 | model.decoder.layers.5.residual_connections.1.dropout | Dropout                 | 0     \n",
      "272 | model.decoder.layers.5.residual_connections.1.norm    | LayerNormalization      | 2     \n",
      "273 | model.decoder.layers.5.residual_connections.2         | ResidualConnection      | 2     \n",
      "274 | model.decoder.layers.5.residual_connections.2.dropout | Dropout                 | 0     \n",
      "275 | model.decoder.layers.5.residual_connections.2.norm    | LayerNormalization      | 2     \n",
      "276 | model.decoder.norm                                    | LayerNormalization      | 2     \n",
      "277 | model.src_embed                                       | InputEmbeddings         | 8.0 M \n",
      "278 | model.src_embed.embedding                             | Embedding               | 8.0 M \n",
      "279 | model.tgt_embed                                       | InputEmbeddings         | 11.5 M\n",
      "280 | model.tgt_embed.embedding                             | Embedding               | 11.5 M\n",
      "281 | model.src_pos                                         | PositionalEncoding      | 0     \n",
      "282 | model.src_pos.dropout                                 | Dropout                 | 0     \n",
      "283 | model.tgt_pos                                         | PositionalEncoding      | 0     \n",
      "284 | model.tgt_pos.dropout                                 | Dropout                 | 0     \n",
      "285 | model.projection_layer                                | ProjectionLayer         | 11.5 M\n",
      "286 | model.projection_layer.proj                           | Linear                  | 11.5 M\n",
      "287 | loss_fn                                               | CrossEntropyLoss        | 0     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "75.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "75.1 M    Total params\n",
      "300.532   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f442420218240ed88cdbd816490c6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of source sentence: 309\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of source sentence: 309\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of target sentence: 274\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of target sentence: 274\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: Dolly, gazing beyond her sister-in-law, listened thoughtfully.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: Dolly, gazing beyond her sister-in-law, listened thoughtfully.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Dolly guardava pensosa al di là della cognata, ascoltando le sue parole.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Dolly guardava pensosa al di là della cognata, ascoltando le sue parole.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Dolly guardava la sorella di sotto la sorella , sorrise .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Dolly guardava la sorella di sotto la sorella , sorrise .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of source sentence: 309\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of source sentence: 309\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max length of target sentence: 274\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max length of target sentence: 274\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: 'I will come when you are married,' said Varenka.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: 'I will come when you are married,' said Varenka.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: — Verrò quando vi sposerete — disse Varen’ka.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: — Verrò quando vi sposerete — disse Varen’ka.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): — Io ti vado quando tu sei sposata — disse Varen ’ ka .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): — Io ti vado quando tu sei sposata — disse Varen ’ ka .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a0d80b19c4b0e9ad5b83ed57813d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: \"You know--and perhaps think well of.\"\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: \"You know--and perhaps think well of.\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: — Voi pensate forse....\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: — Voi pensate forse....\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): — Sapete , e forse forse ?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): — Sapete , e forse forse ?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e458d6ff494b079ad081bd496ec086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: The children, with frightened and joyful yells, ran on in front.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: The children, with frightened and joyful yells, ran on in front.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: I bambini correvano avanti con uno stridio spaventato e gioioso.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: I bambini correvano avanti con uno stridio spaventato e gioioso.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): I bambini , e di fuori , si verso la scala .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): I bambini , e di fuori , si verso la scala .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66592e5ea094fd2b24f08c615b34dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SOURCE: Besides the sewing of little shirts and the knitting of swaddling bands, on which they were all engaged, \n",
       "to-day jam was being made there in a way new to Agatha Mikhaylovna: without the addition of water to the fruit.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SOURCE: Besides the sewing of little shirts and the knitting of swaddling bands, on which they were all engaged, \n",
       "to-day jam was being made there in a way new to Agatha Mikhaylovna: without the addition of water to the fruit.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TARGET: Oltre la confezione delle camicine e delle fasce a maglia, di cui tutte si occupavano, quel giorno si \n",
       "confezionava la marmellata secondo un metodo nuovo per Agaf’ja Michajlovna, senza, cioè, aggiungervi acqua. Kitty \n",
       "introduceva questo metodo nuovo usato in casa sua.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TARGET: Oltre la confezione delle camicine e delle fasce a maglia, di cui tutte si occupavano, quel giorno si \n",
       "confezionava la marmellata secondo un metodo nuovo per Agaf’ja Michajlovna, senza, cioè, aggiungervi acqua. Kitty \n",
       "introduceva questo metodo nuovo usato in casa sua.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PREDICTED (Lavanya Nemani &amp; Shashank Gupta): Inoltre il gran e i di , su quello di cui erano stati tutti i , il \n",
       "frumento si in modo un nuovo nuovo nuovo nuovo , senza volere Agaf ’ ja Michajlovna .\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PREDICTED (Lavanya Nemani & Shashank Gupta): Inoltre il gran e i di , su quello di cui erano stati tutti i , il \n",
       "frumento si in modo un nuovo nuovo nuovo nuovo , senza volere Agaf ’ ja Michajlovna .\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    callbacks=[ModelSummary(max_depth=-1), RichProgressBar(leave=True)],\n",
    "    accelerator=\"gpu\", devices=[1],\n",
    "    max_epochs = 4,\n",
    "    )\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Transformer_epoch14_Lavanya_Shashank.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
