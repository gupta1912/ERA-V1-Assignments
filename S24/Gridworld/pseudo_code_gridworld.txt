1. __init__
initializes the q-values of the system to empty dictionary self.qvalues 

2. getQValue
retrieve the q-value of a state and action if it is self.qvalues else return q-value = 0.0

3. computeValueFromQValue
value of a state, action is the max of all q-values of that state, possible actions.  
    A. get the q-values for each possible action if it is allowed by checking if action is in self.getLegalActions(state)
    B. max of above q-values is the computeValueFromQValue output. 
    C. if no possible actions can be taken the computeValueFromQValue = 0.0

4. computeActionFromQValues
for a given state, action is taken by deciding which action gives the max-q-value
    A. first find the best q-value which is simply the value of the state, action = V using computeValueFromQValue
    B. find all possible actions in that state found using actions in self.getLegalActions(state)
    C. find which of these actions gives the max-q-value using getQValue(state, action)
    D. if there are multiple actions which give the max-q-value, we chose an action randomly from them
    E. if there are no possible actions, we don't return action. 

5. getAction
action taken at any state are determined by policy and epsilon (which adds randomness to the process)
    A. if random probability <  epsilon, then we chose a random action from all possible actions in that state
    B. Otherwise, we chose the action dictated by the policy using self.getPolicy(state)

6. update
function to update the q-values during q-learning. state, action, nextState, reward
    A. qvalue_new is set as = qvalue + alpha * (reward + disc * next_value - qvalue)
    B. where alpha is analogous to learning rate, reward is set by environment,  discount factor is discounts future rewards. 